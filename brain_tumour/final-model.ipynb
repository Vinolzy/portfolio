{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3596fcf",
   "metadata": {
    "id": "CuEOx57tnjoo",
    "papermill": {
     "duration": 0.004392,
     "end_time": "2025-05-05T15:26:49.921640",
     "exception": false,
     "start_time": "2025-05-05T15:26:49.917248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Needed Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f12efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:26:49.932645Z",
     "iopub.status.busy": "2025-05-05T15:26:49.932336Z",
     "iopub.status.idle": "2025-05-05T15:27:16.440864Z",
     "shell.execute_reply": "2025-05-05T15:27:16.440250Z"
    },
    "id": "3BPS_Cgknjor",
    "papermill": {
     "duration": 26.51619,
     "end_time": "2025-05-05T15:27:16.442318",
     "exception": false,
     "start_time": "2025-05-05T15:26:49.926128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 15:27:04.509655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746458824.744974      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746458824.812368      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "# from torchvision.models import resnet18, VGG16_Weights, ResNet18_Weights, AlexNet_Weights\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import math\n",
    "from typing import Optional, Tuple, List, Dict, Any\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2645de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:16.600398Z",
     "iopub.status.busy": "2025-05-05T15:27:16.600151Z",
     "iopub.status.idle": "2025-05-05T15:27:16.688553Z",
     "shell.execute_reply": "2025-05-05T15:27:16.687776Z"
    },
    "id": "vtrgujlwnjos",
    "outputId": "aff5381f-f421-44cb-dd23-e29596731a1a",
    "papermill": {
     "duration": 0.093172,
     "end_time": "2025-05-05T15:27:16.689709",
     "exception": false,
     "start_time": "2025-05-05T15:27:16.596537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the device for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30cd4d",
   "metadata": {
    "id": "TmwKAIHXnjot",
    "papermill": {
     "duration": 0.003807,
     "end_time": "2025-05-05T15:27:16.696627",
     "exception": false,
     "start_time": "2025-05-05T15:27:16.692820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60761cee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:16.703124Z",
     "iopub.status.busy": "2025-05-05T15:27:16.702898Z",
     "iopub.status.idle": "2025-05-05T15:27:28.434142Z",
     "shell.execute_reply": "2025-05-05T15:27:28.433294Z"
    },
    "id": "sfnlwRlpnjou",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 11.73623,
     "end_time": "2025-05-05T15:27:28.435562",
     "exception": false,
     "start_time": "2025-05-05T15:27:16.699332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline: RGB\n",
    "train_data_dataloader_baseline = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n",
    "    ], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_data_dataloader_baseline = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom: RGB\n",
    "train_data_dataloader = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    #transforms.RandomApply([\n",
    "        #transforms.RandomRotation(degrees=15),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n",
    "    #], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_data_dataloader = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom: Gray\n",
    "train_data_dataloader_gry = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.2),\n",
    "    #transforms.RandomApply([\n",
    "        #transforms.RandomRotation(degrees=15),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1))\n",
    "    #], p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_data_dataloader_gry = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# dataset: https://drive.google.com/drive/folders/1d1ArqNswahfsmP6h-fm3WCvL47dVOpxA?usp=drive_link\n",
    "\n",
    "# Define the base directory for data\n",
    "base_data_dir = '/input/brain-tumor-mri-dataset'\n",
    "\n",
    "# 1. Baseline: RGB\n",
    "# train_dataset_base = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader_baseline)\n",
    "# train_loader_base = DataLoader(train_dataset_base, batch_size=128, shuffle=True)  # batch_size set to 128 due to \"CUDA out of memory\" when using VGG16 model\n",
    "\n",
    "# val_dataset_base = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader_baseline)\n",
    "# val_loader_base = DataLoader(val_dataset_base, batch_size=128, shuffle=False)\n",
    "\n",
    "# 2. Custom: RGB\n",
    "train_dataset = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 3. Custom: Gray\n",
    "train_dataset_gry = ImageFolder(f'{base_data_dir}/Training', transform=train_data_dataloader_gry)\n",
    "train_loader_gry = DataLoader(train_dataset_gry, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset_gry = ImageFolder(f'{base_data_dir}/Testing', transform=val_data_dataloader_gry)\n",
    "val_loader_gry = DataLoader(val_dataset_gry, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e6ea4",
   "metadata": {
    "id": "M39WRjADnjox",
    "papermill": {
     "duration": 0.002627,
     "end_time": "2025-05-05T15:27:28.441366",
     "exception": false,
     "start_time": "2025-05-05T15:27:28.438739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Custom Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eff78da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:28.448616Z",
     "iopub.status.busy": "2025-05-05T15:27:28.448398Z",
     "iopub.status.idle": "2025-05-05T15:27:28.466293Z",
     "shell.execute_reply": "2025-05-05T15:27:28.465699Z"
    },
    "id": "f_aCuXcvnjox",
    "papermill": {
     "duration": 0.023029,
     "end_time": "2025-05-05T15:27:28.467334",
     "exception": false,
     "start_time": "2025-05-05T15:27:28.444305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Use a single projection for QKV to reduce computation\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, C = x.shape\n",
    "\n",
    "        # Efficient QKV projection\n",
    "        qkv = self.qkv_proj(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.view(B, L, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n",
    "\n",
    "        # Use einsum for more efficient attention computation\n",
    "        attn_weights = torch.einsum('bhqd,bhkd->bhqk', q, k) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        context = torch.einsum('bhqk,bhkd->bhqd', attn_probs, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, C)\n",
    "\n",
    "        return self.out_proj(context)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.attention = EfficientMultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            #nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        attn_out = self.attention(self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        mlp_out = self.mlp(self.norm2(x))\n",
    "        x = x + mlp_out\n",
    "        return x\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = x + self.attention(x)\n",
    "    #     x = x + self.mlp(x)\n",
    "    #     return x\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (base ** (torch.arange(0., dim, 2.) / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1):\n",
    "        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)\n",
    "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "class HybridFPNTransformer_gry(nn.Module):\n",
    "    def __init__(self, num_classes, image_size=224, patch_size=16,\n",
    "                 embed_dim=128, num_heads=8, transformer_depth=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem network remains the same - processes raw images\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(1, embed_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim*2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Patch embedding remains the same\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            embed_dim, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        # Rotary positional embeddings for spatial awareness\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(embed_dim)\n",
    "\n",
    "        # Transformer blocks remain unchanged\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, embed_dim)\n",
    "            for _ in range(transformer_depth)\n",
    "        ])\n",
    "\n",
    "        # Simplified classification head - directly from transformer output\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x = self.stem(x)\n",
    "        x_patches = self.patch_embed(x)\n",
    "\n",
    "        # Reshape for transformer processing\n",
    "        B, C, H, W = x_patches.shape\n",
    "        x_patches = x_patches.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Add positional information\n",
    "        rotary_pos_emb = self.rotary_emb(x_patches)\n",
    "        x_patches = x_patches + rotary_pos_emb\n",
    "\n",
    "        # Process through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x_patches = transformer_block(x_patches)\n",
    "\n",
    "        # Reshape back to 2D and classify\n",
    "        x_transformed = x_patches.transpose(1, 2).view(B, -1, H, W)\n",
    "        return self.classifier(x_transformed)\n",
    "\n",
    "\n",
    "class HybridFPNTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, image_size=224, patch_size=16,\n",
    "                 embed_dim=128, num_heads=8, transformer_depth=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem network remains the same - processes raw images\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, embed_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(embed_dim*2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Patch embedding remains the same\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            embed_dim, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        # Rotary positional embeddings for spatial awareness\n",
    "        self.rotary_emb = RotaryPositionalEmbedding(embed_dim)\n",
    "\n",
    "        # Transformer blocks remain unchanged\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, embed_dim)\n",
    "            for _ in range(transformer_depth)\n",
    "        ])\n",
    "\n",
    "        # Simplified classification head - directly from transformer output\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial feature extraction\n",
    "        x = self.stem(x)\n",
    "        x_patches = self.patch_embed(x)\n",
    "\n",
    "        # Reshape for transformer processing\n",
    "        B, C, H, W = x_patches.shape\n",
    "        x_patches = x_patches.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Add positional information\n",
    "        rotary_pos_emb = self.rotary_emb(x_patches)\n",
    "        x_patches = x_patches + rotary_pos_emb\n",
    "\n",
    "        # Process through transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x_patches = transformer_block(x_patches)\n",
    "\n",
    "        # Reshape back to 2D and classify\n",
    "        x_transformed = x_patches.transpose(1, 2).view(B, -1, H, W)\n",
    "        return self.classifier(x_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10bccf54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:28.473649Z",
     "iopub.status.busy": "2025-05-05T15:27:28.473439Z",
     "iopub.status.idle": "2025-05-05T15:27:28.494498Z",
     "shell.execute_reply": "2025-05-05T15:27:28.493959Z"
    },
    "id": "hSdh0S5iHbq-",
    "papermill": {
     "duration": 0.025422,
     "end_time": "2025-05-05T15:27:28.495472",
     "exception": false,
     "start_time": "2025-05-05T15:27:28.470050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_logging(\n",
    "    log_dir: str,\n",
    "    log_level: int = logging.INFO,\n",
    "    verbose: bool = False\n",
    ") -> logging.Logger:\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Generate more detailed log filename\n",
    "    log_filename = os.path.join(\n",
    "        log_dir,\n",
    "        f'tumor_classification_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'\n",
    "    )\n",
    "\n",
    "    # Configure logging with more details\n",
    "    logging.basicConfig(\n",
    "        level=log_level,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s: %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logger = logging.getLogger('TumorClassification')\n",
    "\n",
    "    # Add hardware information logging\n",
    "    if verbose:\n",
    "        logger.info(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "        logger.info(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "def calculate_metrics(true_labels, pred_labels, num_classes):\n",
    "    # Calculate metrics with weighted average for multiclass\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "    # Sensitivity (Per Class) derived from confusion matrix\n",
    "    sensitivity = []\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]  # True positives for class i\n",
    "        fn = cm[i, :].sum() - tp  # False negatives for class i\n",
    "        sensitivity.append(tp / (tp + fn) if (tp + fn) > 0 else 0)\n",
    "\n",
    "    return {\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'sensitivity': sensitivity,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "def save_metrics_to_csv(train_metrics, val_metrics, csv_path):\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header\n",
    "        headers = ['Epoch', 'Train_F1', 'Train_Precision', 'Train_Recall', \n",
    "                   'Train_Sensitivity', 'Train_Confusion_Matrix',\n",
    "                   'Val_F1', 'Val_Precision', 'Val_Recall', \n",
    "                   'Val_Sensitivity', 'Val_Confusion_Matrix']\n",
    "        writer.writerow(headers)\n",
    "        \n",
    "        # Write metrics for each epoch\n",
    "        for epoch, (train, val) in enumerate(zip(train_metrics, val_metrics), start=1):\n",
    "            writer.writerow([\n",
    "                epoch,\n",
    "                train['f1_score'],\n",
    "                train['precision'],\n",
    "                train['recall'],\n",
    "                train['sensitivity'],\n",
    "                str(train['confusion_matrix'].tolist()),  # Convert to string\n",
    "                val['f1_score'],\n",
    "                val['precision'],\n",
    "                val['recall'],\n",
    "                val['sensitivity'],\n",
    "                str(val['confusion_matrix'].tolist())  # Convert to string\n",
    "            ])\n",
    "    print(f\"Metrics saved to {csv_path}\")\n",
    "\n",
    "def plot_metrics(train_metrics, val_metrics, save_dir):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare metrics for plotting\n",
    "    epochs = range(1, len(train_metrics) + 1)\n",
    "    metrics_to_plot = ['f1_score', 'precision', 'recall', 'sensitivity']\n",
    "\n",
    "    for metric in metrics_to_plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        train_values = [epoch_metrics[metric] if metric != 'sensitivity' else np.mean(epoch_metrics[metric]) for epoch_metrics in train_metrics]\n",
    "        val_values = [epoch_metrics[metric] if metric != 'sensitivity' else np.mean(epoch_metrics[metric]) for epoch_metrics in val_metrics]\n",
    "\n",
    "        plt.plot(epochs, train_values, label=f'Train {metric}', marker='o')\n",
    "        plt.plot(epochs, val_values, label=f'Validation {metric}', marker='o')\n",
    "        \n",
    "        plt.title(f'{metric.capitalize()} over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        # Save the plot as an image\n",
    "        save_path = os.path.join(save_dir, f'{metric}.png')\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Saved plot for {metric} to {save_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "def train_model_enhanced(\n",
    "    model: nn.Module,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int = 150,\n",
    "    device: Optional[torch.device] = None,\n",
    "    num_classes: int = 4,\n",
    "    log_dir: str = './logs',\n",
    "    checkpoint_dir: str = '/working',\n",
    "    model_name  = None,\n",
    "    early_stopping_patience: int = 10,\n",
    "    learning_rate_patience: int = 5\n",
    ") -> Tuple[nn.Module, List[Dict], List[Dict]]:\n",
    "\n",
    "    # Device management\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_name)\n",
    "    log_dir = os.path.join(log_dir, model_name)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Logging and TensorBoard\n",
    "    logger = setup_logging(log_dir, verbose=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=learning_rate_patience\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_metrics, val_metrics = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds, train_true = [], []\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Optional gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_true = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Metrics calculation (similar to original function)\n",
    "        train_metrics_epoch = calculate_metrics(\n",
    "            np.array(train_true),\n",
    "            np.array(train_preds),\n",
    "            num_classes\n",
    "        )\n",
    "        val_metrics_epoch = calculate_metrics(\n",
    "            np.array(val_true),\n",
    "            np.array(val_preds),\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "        # TensorBoard logging\n",
    "        writer.add_scalar('Loss/Train', train_loss/len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss/len(val_loader), epoch)\n",
    "        writer.add_scalar('F1/Train', train_metrics_epoch['f1_score'], epoch)\n",
    "        writer.add_scalar('F1/Validation', val_metrics_epoch['f1_score'], epoch)\n",
    "        writer.add_scalar('Precision/Train', train_metrics_epoch['precision'], epoch)\n",
    "        writer.add_scalar('Precision/Validation', val_metrics_epoch['precision'], epoch)\n",
    "        writer.add_scalar('Recall/Train', train_metrics_epoch['recall'], epoch)\n",
    "        writer.add_scalar('Recall/Validation', val_metrics_epoch['recall'], epoch)\n",
    "        writer.add_scalar('Sensitivity/Train', np.mean(train_metrics_epoch['sensitivity']), epoch)\n",
    "        writer.add_scalar('Sensitivity/Validation', np.mean(val_metrics_epoch['sensitivity']), epoch)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_metrics_epoch['f1_score'])\n",
    "        print(val_metrics_epoch['f1_score'])\n",
    "        # Early stopping\n",
    "        if val_metrics_epoch['f1_score'] > best_val_f1 and epoch >=10:\n",
    "            best_val_f1 = val_metrics_epoch['f1_score']\n",
    "            #print(best_val_f1)\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_f1': best_val_f1,\n",
    "                'epoch': epoch\n",
    "            }, os.path.join(checkpoint_dir, f'{model_name}_{epoch}_f1_{best_val_f1:.4f}.pth'))\n",
    "\n",
    "\n",
    "        # Log epoch summary\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        logger.info(f\"Train F1: {train_metrics_epoch['f1_score']:.4f}\")\n",
    "        logger.info(f\"Val F1: {val_metrics_epoch['f1_score']:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if epochs_no_improve >= early_stopping_patience:\n",
    "            logger.info(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "        train_metrics.append(train_metrics_epoch)\n",
    "        val_metrics.append(val_metrics_epoch)\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    return model, train_metrics, val_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5247ca52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:28.501867Z",
     "iopub.status.busy": "2025-05-05T15:27:28.501615Z",
     "iopub.status.idle": "2025-05-05T15:27:28.512759Z",
     "shell.execute_reply": "2025-05-05T15:27:28.512200Z"
    },
    "papermill": {
     "duration": 0.015833,
     "end_time": "2025-05-05T15:27:28.513986",
     "exception": false,
     "start_time": "2025-05-05T15:27:28.498153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Function to find highest F1 score files\n",
    "def find_highest_f1_files():\n",
    "    specific_folders = [\n",
    "        {\"folder\": \"VGG16\", \"loader\": val_loader_base},\n",
    "        {\"folder\": \"AlexNet\", \"loader\": val_loader_base},\n",
    "        {\"folder\": \"ResNet18\", \"loader\": val_loader_base},\n",
    "        {\"folder\": \"HybridFPNTransformer\", \"loader\": val_loader},\n",
    "        {\"folder\": \"HybridFPNTransformerGRY\", \"loader\": val_loader_gry}\n",
    "    ]\n",
    "\n",
    "    highest_f1_files = []\n",
    "\n",
    "    for item in specific_folders:\n",
    "        folder_name = item[\"folder\"]\n",
    "        loader = item[\"loader\"]\n",
    "        folder_path = os.path.join(\"/working\", folder_name)\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            max_f1 = -1\n",
    "            max_file = \"\"\n",
    "            max_epoch = -1\n",
    "\n",
    "            # Iterate through each file in the folder\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Use regex to extract the epoch and F1 score from the file name\n",
    "                match = re.search(r\"best_\\w+_(\\d+)_f1_(\\d+\\.\\d+)\\.pth\", file_name)\n",
    "                if match:\n",
    "                    epoch = int(match.group(1))\n",
    "                    f1_score = float(match.group(2))\n",
    "\n",
    "                    # Update the highest F1 score and corresponding file path\n",
    "                    if f1_score > max_f1 or (f1_score == max_f1 and epoch > max_epoch):\n",
    "                        max_f1 = f1_score\n",
    "                        max_file = file_path\n",
    "                        max_epoch = epoch\n",
    "\n",
    "            if max_file:\n",
    "                highest_f1_files.append({\"file\": max_file, \"loader\": loader})\n",
    "\n",
    "    return highest_f1_files\n",
    "\n",
    "def evaluate_model(model_path, dataloader, device):\n",
    "    # Determine the model architecture based on the file path\n",
    "    if \"VGG16\" in model_path:\n",
    "        model = models.vgg16(pretrained=False)\n",
    "        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 4)  # Adjust for 4 classes\n",
    "    elif \"ResNet18\" in model_path:\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 4)  # Adjust for 4 classes\n",
    "    elif \"AlexNet\" in model_path:\n",
    "        model = models.alexnet(pretrained=False)\n",
    "        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 4)  # Adjust for 4 classes\n",
    "    elif \"HybridFPNTransformerGRY\" in model_path:\n",
    "        model = HybridFPNTransformer_gry(num_classes=4)\n",
    "    else:  # Default to HybridFPNTransformer\n",
    "        model = HybridFPNTransformer(num_classes=4)\n",
    "\n",
    "    # Load the model weights from the checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.to(device)  # Move model to the specified device\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    inference_times = []\n",
    "\n",
    "    # Iterate through the dataset\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Record start time\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Perform inference\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Record end time\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Calculate inference time\n",
    "            inference_times.append(end_time - start_time)\n",
    "\n",
    "            # Update accuracy metrics\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate average inference time and accuracy\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_inference_time, accuracy\n",
    "\n",
    "\n",
    "def inference():\n",
    "    # Default highest_f1_files\n",
    "    highest_f1_files = [\n",
    "        {'file': '/input/inference/pytorch/default/1/best_VGG16_51_f1_0.9977.pth', 'loader': val_loader_base},\n",
    "        {'file': '/input/inference/pytorch/default/1/best_AlexNet_53_f1_0.9931.pth', 'loader': val_loader_base},\n",
    "        {'file': '/input/inference/pytorch/default/1/best_ResNet18_50_f1_0.9970.pth', 'loader': val_loader_base},\n",
    "        {'file': '/input/inference/pytorch/default/1/best_HybridFPNTransformer_54_f1_0.9809.pth', 'loader': val_loader},\n",
    "        {'file': '/input/inference/pytorch/default/1/best_HybridFPNTransformerGRY_67_f1_0.9847.pth', 'loader': val_loader_gry}\n",
    "    ]\n",
    "    # highest_f1_files = find_highest_f1_files()\n",
    "    print(\"highest_f1_files\", highest_f1_files)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    for item in highest_f1_files:\n",
    "        model_path = item[\"file\"]\n",
    "        dataloader = item[\"loader\"]\n",
    "        avg_inference_time, accuracy = evaluate_model(model_path, dataloader, device)\n",
    "        print(f\"Model: {model_path}\")\n",
    "        print(f\"Average Inference Time: {avg_inference_time:.4f} seconds\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe2d00f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T15:27:28.520489Z",
     "iopub.status.busy": "2025-05-05T15:27:28.520289Z",
     "iopub.status.idle": "2025-05-05T19:57:50.507219Z",
     "shell.execute_reply": "2025-05-05T19:57:50.506508Z"
    },
    "id": "VXTmnJ7OTEq5",
    "papermill": {
     "duration": 16221.992015,
     "end_time": "2025-05-05T19:57:50.508697",
     "exception": false,
     "start_time": "2025-05-05T15:27:28.516682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6434079745877987\n",
      "0.6943618148260734\n",
      "0.7229233605983775\n",
      "0.7514865149849943\n",
      "0.725989576520661\n",
      "0.78371563767246\n",
      "0.8223111660508293\n",
      "0.8314123094583606\n",
      "0.8411361745111943\n",
      "0.851175348315338\n",
      "0.8618600169824084\n",
      "0.8969221000369552\n",
      "0.883098443750513\n",
      "0.8954407886028328\n",
      "0.9019701007568945\n",
      "0.9228700748866195\n",
      "0.8922961451608985\n",
      "0.9222732195239202\n",
      "0.8682662149133128\n",
      "0.8912527041363546\n",
      "0.9371199223468786\n",
      "0.943616063403138\n",
      "0.9664044783418088\n",
      "0.9611867522699907\n",
      "0.9616767257144553\n",
      "0.9423009083778476\n",
      "0.9501262839924063\n",
      "0.9010354533213821\n",
      "0.9579420873653909\n",
      "0.9700972298332052\n",
      "0.9679993385923079\n",
      "0.9724971666980311\n",
      "0.9620803330326074\n",
      "0.9708063826691773\n",
      "0.9778655093902092\n",
      "0.9677122460069181\n",
      "0.9521909698193002\n",
      "0.9582615803779487\n",
      "0.9681131711269303\n",
      "0.9695758389882516\n",
      "0.9693405309936683\n",
      "0.9739853188900299\n",
      "0.9777933088844777\n",
      "0.9685692331892316\n",
      "0.9747465087930427\n",
      "0.9754809716805165\n",
      "0.9708403345517345\n",
      "0.9754662899886485\n",
      "0.9739420744738877\n",
      "0.9747507776730139\n",
      "0.9756053907586271\n",
      "0.9754976986030446\n",
      "0.9755690374044896\n",
      "0.9763067449911808\n",
      "0.9755182007657601\n",
      "0.9747294421325673\n",
      "0.9739600747190514\n",
      "0.9739600747190514\n",
      "0.9739600747190514\n",
      "0.9747310460213736\n",
      "0.9754996625233696\n",
      "0.9747310460213736\n",
      "0.9739389494378482\n",
      "0.9754983360899336\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.973171716185002\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9762670077774113\n",
      "0.9747293626986466\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9747293626986466\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9747293626986466\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9770360468260784\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9747310460213736\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9747293626986466\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9762679918357108\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9754996625233696\n",
      "0.9754996625233696\n",
      "0.9762679918357108\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9762679918357108\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9754996625233696\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "0.9739621294458004\n",
      "Metrics saved to ./logs/Transformer_metrics.csv\n",
      "Saved plot for f1_score to ./logs/Transformer_plots.png/f1_score.png\n",
      "Saved plot for precision to ./logs/Transformer_plots.png/precision.png\n",
      "Saved plot for recall to ./logs/Transformer_plots.png/recall.png\n",
      "Saved plot for sensitivity to ./logs/Transformer_plots.png/sensitivity.png\n",
      "0.540064457809238\n",
      "0.6968939879524659\n",
      "0.7186494261530706\n",
      "0.7145988366591766\n",
      "0.7616869337797605\n",
      "0.7886578863423043\n",
      "0.8189370260447807\n",
      "0.8224846879546733\n",
      "0.8231204731489825\n",
      "0.8504618498080594\n",
      "0.8553857468427899\n",
      "0.8823215464351944\n",
      "0.9017464262794167\n",
      "0.8458769834312404\n",
      "0.8787862552394435\n",
      "0.8831348899427763\n",
      "0.9080150375067385\n",
      "0.9061966720103406\n",
      "0.9242631568905217\n",
      "0.8966210954428537\n",
      "0.9352953523575195\n",
      "0.9424953407638544\n",
      "0.9169272559564295\n",
      "0.9564762709242125\n",
      "0.940182375881418\n",
      "0.9638128252294083\n",
      "0.9677007602428372\n",
      "0.9633914363528655\n",
      "0.9452727770235407\n",
      "0.9630919145657098\n",
      "0.9669950569716075\n",
      "0.9473723597103154\n",
      "0.9511194929305394\n",
      "0.970068361361494\n",
      "0.9669649807374058\n",
      "0.9746883613212379\n",
      "0.966224101547472\n",
      "0.9740077481274131\n",
      "0.9732223997043773\n",
      "0.9755229406833296\n",
      "0.96311891527447\n",
      "0.9754796141753007\n",
      "0.9677419900694334\n",
      "0.9677361162299457\n",
      "0.9801737756382397\n",
      "0.9738522269676959\n",
      "0.9793692944358001\n",
      "0.9808976130716581\n",
      "0.9739540103517704\n",
      "0.9808592280177629\n",
      "0.9808623468248975\n",
      "0.9809013426022276\n",
      "0.9754940713341115\n",
      "0.9692769450711628\n",
      "0.9755123592124371\n",
      "0.9785838786363396\n",
      "0.9793689001585294\n",
      "0.9808859223992514\n",
      "0.9709112558093342\n",
      "0.9762706424957089\n",
      "0.9762850031194787\n",
      "0.9808858894024286\n",
      "0.9770582570196944\n",
      "0.9762920202524241\n",
      "0.9785916188825422\n",
      "0.9801690021148357\n",
      "0.9778264951439203\n",
      "0.9762877265458209\n",
      "0.9785888597849556\n",
      "0.9785939910824578\n",
      "0.9763135656691913\n",
      "0.9755182028943633\n",
      "0.9762877265458209\n",
      "0.977054582488751\n",
      "0.9755182028943633\n",
      "0.9755283109756098\n",
      "0.9755182028943633\n",
      "0.9755182028943633\n",
      "0.9755016726159705\n",
      "0.9755016726159705\n",
      "0.9747155479452431\n",
      "0.9762683129550094\n",
      "0.9778211740505622\n",
      "0.9770379463802613\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.977054582488751\n",
      "0.9762877265458209\n",
      "0.9762877265458209\n",
      "0.9762877265458209\n",
      "0.9762877265458209\n",
      "0.9778211740505622\n",
      "0.977054582488751\n",
      "0.977054582488751\n",
      "0.977054582488751\n",
      "0.9755182028943633\n",
      "0.977054582488751\n",
      "0.9785875140071206\n",
      "0.977054582488751\n",
      "0.9778211740505622\n",
      "0.9778211740505622\n",
      "0.9762877265458209\n",
      "0.9785875140071206\n",
      "0.977054582488751\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.9785875140071206\n",
      "0.9770515406253104\n",
      "0.9793536151201903\n",
      "0.9762877265458209\n",
      "0.9762850031194787\n",
      "0.9762877265458209\n",
      "0.9785875140071206\n",
      "0.9793500013597664\n",
      "0.9778211740505622\n",
      "0.9778211740505622\n",
      "0.9778178282046465\n",
      "0.9778211740505622\n",
      "0.9778178282046465\n",
      "0.9778211740505622\n",
      "0.9785875140071206\n",
      "0.9770515406253104\n",
      "0.9793536151201903\n",
      "0.9785875140071206\n",
      "0.977054582488751\n",
      "0.9770515406253104\n",
      "0.9793536151201903\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.9778211740505622\n",
      "0.9778211740505622\n",
      "0.9793536151201903\n",
      "0.9778211740505622\n",
      "0.9762850031194787\n",
      "0.9762877265458209\n",
      "0.9785875140071206\n",
      "0.9762850031194787\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.9770515406253104\n",
      "0.9770515406253104\n",
      "0.9778211740505622\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.9770515406253104\n",
      "0.9785875140071206\n",
      "0.9785875140071206\n",
      "0.9793536151201903\n",
      "0.9778178282046465\n",
      "0.977054582488751\n",
      "0.9778211740505622\n",
      "0.9778211740505622\n",
      "0.9785875140071206\n",
      "0.9785875140071206\n",
      "0.9778211740505622\n",
      "0.977054582488751\n",
      "0.9747511271425242\n",
      "0.9785875140071206\n",
      "0.9785875140071206\n",
      "0.9762850031194787\n",
      "Metrics saved to ./logs/HybridFPNTransformerGRY_metrics.csv\n",
      "Saved plot for f1_score to ./logs/HybridFPNTransformerGRY_plots.png/f1_score.png\n",
      "Saved plot for precision to ./logs/HybridFPNTransformerGRY_plots.png/precision.png\n",
      "Saved plot for recall to ./logs/HybridFPNTransformerGRY_plots.png/recall.png\n",
      "Saved plot for sensitivity to ./logs/HybridFPNTransformerGRY_plots.png/sensitivity.png\n"
     ]
    }
   ],
   "source": [
    "EPOCHS_NUM = 160\n",
    "# EPOCHS_NUM = 1\n",
    "def main():\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Log directory setup\n",
    "    log_dir = './logs/'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # # Define model dictionary\n",
    "    # model_dict = {\n",
    "    #     'VGG16': {\n",
    "    #         'model': models.vgg16(),\n",
    "    #         'weights_path': '/input/model/pytorch/default/1/vgg16-397923af.pth'\n",
    "    #     },\n",
    "    #     'ResNet18': {\n",
    "    #         'model': models.resnet18(),\n",
    "    #         'weights_path': '/input/model/pytorch/default/1/resnet18-f37072fd.pth'\n",
    "    #     },\n",
    "    #     'AlexNet': {\n",
    "    #         'model': models.alexnet(),\n",
    "    #         'weights_path': '/input/model/pytorch/default/1/alexnet-owt-7be5be79.pth'\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    # # CSV to store training time and parameters\n",
    "    time_metrics = []\n",
    "\n",
    "    # # Iterate through base models for experimentation\n",
    "    # for model_name, model_info in model_dict.items():\n",
    "    #     try:\n",
    "    #         # Load pre-trained weights and adjust the classifier layer\n",
    "    #         base_model = model_info['model']\n",
    "    #         weights_path = model_info['weights_path']\n",
    "    #         base_model.load_state_dict(torch.load(weights_path))\n",
    "            \n",
    "    #         if model_name in ['VGG16', 'AlexNet']:\n",
    "    #             base_model.classifier[-1] = nn.Linear(base_model.classifier[-1].in_features, 4)  # Adjust for 4 classes\n",
    "    #         elif model_name == 'ResNet18':\n",
    "    #             base_model.fc = nn.Linear(base_model.fc.in_features, 4)  # Adjust for 4 classes\n",
    "            \n",
    "    #         base_model = base_model.to(device)\n",
    "\n",
    "    #         # Define loss and optimizer\n",
    "    #         criterion = nn.CrossEntropyLoss()\n",
    "    #         optimizer = optim.Adam(\n",
    "    #             base_model.parameters(),\n",
    "    #             lr=1e-4,\n",
    "    #             weight_decay=1e-5  # L2 regularization\n",
    "    #         )\n",
    "\n",
    "    #         # Record start time\n",
    "    #         start_time = time.time()\n",
    "\n",
    "    #         # Train the model\n",
    "    #         trained_model, train_metrics, val_metrics = train_model_enhanced(\n",
    "    #             model=base_model,\n",
    "    #             train_loader=train_loader_base,  # Assuming this is defined elsewhere\n",
    "    #             val_loader=val_loader_base,      # Assuming this is defined elsewhere\n",
    "    #             criterion=criterion,\n",
    "    #             optimizer=optimizer,\n",
    "    #             num_epochs=EPOCHS_NUM,\n",
    "    #             model_name=model_name,\n",
    "    #             device=device,\n",
    "    #             num_classes=4\n",
    "    #         )\n",
    "\n",
    "    #         # Record end time and calculate training duration\n",
    "    #         end_time = time.time()\n",
    "    #         training_time = end_time - start_time\n",
    "\n",
    "    #         # Save metrics to CSV\n",
    "    #         csv_path = os.path.join(log_dir, f'{model_name}_metrics.csv')\n",
    "    #         save_metrics_to_csv(train_metrics, val_metrics, csv_path)\n",
    "\n",
    "    #         # Save metric plots\n",
    "    #         plot_path = os.path.join(log_dir, f'{model_name}_plots.png')\n",
    "    #         plot_metrics(train_metrics, val_metrics, plot_path)\n",
    "\n",
    "    #         # Log training time and parameters\n",
    "    #         time_metrics.append({\n",
    "    #             'Model': model_name,\n",
    "    #             'Training Time (seconds)': training_time,\n",
    "    #             'Number of Parameters': sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "    #         })\n",
    "\n",
    "    #         # Clear VRAM\n",
    "    #         del base_model, criterion, optimizer, trained_model\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error processing {model_name}: {e}\")\n",
    "\n",
    "    # # # Save training time and parameter metrics to a CSV\n",
    "    time_metrics_path = os.path.join(log_dir, 'training_time_metrics.csv')\n",
    "    # pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n",
    "\n",
    "    # HybridFPNTransformer training\n",
    "    hybrid_model = HybridFPNTransformer(num_classes=4).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        hybrid_model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5  # L2 regularization\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    trained_hybrid_model, hybrid_train_metrics, hybrid_val_metrics = train_model_enhanced(\n",
    "        model=hybrid_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=EPOCHS_NUM,\n",
    "        device=device,\n",
    "        model_name='Transformer',\n",
    "        num_classes=4\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    hybrid_training_time = end_time - start_time\n",
    "\n",
    "    csv_path = './logs/Transformer_metrics.csv'\n",
    "    save_metrics_to_csv(hybrid_train_metrics, hybrid_val_metrics, csv_path)\n",
    "\n",
    "    plot_path = './logs/Transformer_plots.png'\n",
    "    plot_metrics(hybrid_train_metrics, hybrid_val_metrics, plot_path)\n",
    "\n",
    "    # Log HybridFPNTransformer training time and parameters\n",
    "    time_metrics.append({\n",
    "        'Model': 'Transformer',\n",
    "        'Training Time (seconds)': hybrid_training_time,\n",
    "        'Number of Parameters': sum(p.numel() for p in hybrid_model.parameters() if p.requires_grad)\n",
    "    })\n",
    "\n",
    "    # Clear VRAM\n",
    "    del hybrid_model, criterion, optimizer, trained_hybrid_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n",
    "\n",
    "    hybrid_model_gry = HybridFPNTransformer_gry(num_classes=4).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        hybrid_model_gry.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5  # L2 regularization\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    hybrid_model_gry, hybrid_train_metrics_gry, hybrid_val_metrics_gry = train_model_enhanced(\n",
    "        model=hybrid_model_gry,\n",
    "        # train_loader=train_dataset_gry,\n",
    "        train_loader=train_loader_gry,\n",
    "        val_loader=val_loader_gry,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=EPOCHS_NUM,\n",
    "        device=device,\n",
    "        model_name='HybridFPNTransformerGRY',\n",
    "        num_classes=4\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    hybrid_gry_training_time = end_time - start_time\n",
    "\n",
    "    csv_path = './logs/HybridFPNTransformerGRY_metrics.csv'\n",
    "    save_metrics_to_csv(hybrid_train_metrics_gry, hybrid_val_metrics, csv_path)\n",
    "\n",
    "    plot_path = './logs/HybridFPNTransformerGRY_plots.png'\n",
    "    plot_metrics(hybrid_val_metrics_gry, hybrid_val_metrics, plot_path)\n",
    "\n",
    "    # Log HybridFPNTransformer training time and parameters\n",
    "    time_metrics.append({\n",
    "        'Model': 'HybridFPNTransformerGRY',\n",
    "        'Training Time (seconds)': hybrid_gry_training_time,\n",
    "        'Number of Parameters': sum(p.numel() for p in hybrid_model_gry.parameters() if p.requires_grad)\n",
    "    })\n",
    "\n",
    "    # Clear VRAM\n",
    "    # del hybrid_model_gry, criterion, optimizer, hybrid_model_gry\n",
    "    del hybrid_model_gry, criterion, optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pd.DataFrame(time_metrics).to_csv(time_metrics_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    # inference()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1608934,
     "sourceId": 2645886,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 207359,
     "modelInstanceId": 185219,
     "sourceId": 217223,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 207995,
     "modelInstanceId": 185874,
     "sourceId": 217963,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16268.67444,
   "end_time": "2025-05-05T19:57:54.082604",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-05T15:26:45.408164",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
