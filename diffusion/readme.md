# 去噪扩散模型

本项目是一个基于扩散模型(Diffusion Models)的研究与实现集合，涵盖了从基本的U-Net架构出发，逐步构建、优化并扩展扩散生成过程的多个关键技术。项目中的每个文件都代表了不同方向的实验与改进，最终通过MNIST数据集进行完整复现与图像生成。

## 项目结构

- **Diffusion_Models.ipynb**  
  
  从零开始实现了一个扩散模型，构建了前向加噪与逆向去噪流程，使用U-Net作为基础模型进行噪声预测，并基于Sinusoidal时间嵌入与classifier-free guidance技术完成了完整的无条件图像生成流程。
  
- **Optimizations.ipynb**  
  
  针对图像生成过程中的“棋盘格伪影”问题进行了分析和优化。通过加入skip-connection、调整upsampling/downsampling模块等方式，显著提升了图像生成质量，展示了扩散模型中的工程细节对结果的影响。
  
- **Classifier_Free_Diffusion.ipynb**  
  
  探索了无分类器引导(`Classifier-Free Guidance`)技术，结合有条件和无条件的扩散模型预测，增强了模型对目标类别的可控性，是当前主流文本-图像生成模型的重要组成技术之一。
  
- **CLIP.ipynb**  
  
  结合OpenAI的CLIP模型，将文本与图像编码至同一向量空间中，用于提升图文匹配能力，为后续的“文本驱动的图像生成”打下基础。项目中演示了图像-文本的相似度计算、Top-k检索及交叉对比等功能。
  
- **mnist.ipynb**  
  
  利用上述技术在MNIST数据集上完成端到端的扩散图像生成。实现了完整的数据预处理、模型构建、训练与采样流程，生成了不同数字类别的图像，展示了扩散模型的基本原理与可控生成能力。

